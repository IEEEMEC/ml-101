# Artificial Intelligence
It's hard to picture what exactly AI can do. The idea of someone curing cancer, or visiting Mars, is a
powerful image. But if someone told you there was one single technique that could cure every disease,
take us to Mars, to Jupiter, to Alpha Centauri, that could create the grand unified theory, end hunger,
end aging, make everyone intelligent, crack the genetic code - you get the point - it'd just sound like
they were trying to sell you something.
But we do already have one thing that cured smallpox and took us to the Moon and created science.
Sure, we don't fully understand what that thing is yet. But that's our fault, not some quality of
intelligence that makes it not-understandable. Lightning seemed like it was beyond our understanding
and control before electricity. And our understanding of intelligence only increases over time. And that
which we understand, we can create.
Intelligence is the source of technology. What happens when we use that technology to better our
intelligence? We could create better scientific theories, medicines, movies, but most of all, we could
use it to take our intelligence even further. It is an intelligence explosion, better known as the
Singularity.
The next few decades have become quite probably the most important in human history and its future
and AI the last invention that humanity needs to make.
Godel Escher Bach written by Douglas Hofstadter is the greatest book ever written on the nature of
intelligence, and a very entertaining read about music, art, philosophy, surrealism, literature,
programming, and literally everything.
## Human Bias
When you think of AI destroying humanity, you probably think of Skynet. After all, Terminator is the
most famous representation of evil artificial intelligence in the media. But Arnold Schwarzenegger is
not what an AI would be like. He's not even close. Nor is Ultron, or Agent Smith. HAL 9000 from 2001:
A Space Odyssey and Ava from Ex Machina are closer to reality, but even then only relatively.
An unfriendly AI would not 'rise up' against humanity because it dislikes being controlled by those it
views inferior. Remember, intelligence, in its purest form, is nothing more than optimized thinking for a
goal. Humans only seem more complicated because we have more than a few goals at the same time,
and our optimization process isn't that great. What an AI wants, what it likes and dislikes, are all things
we define for it.
An AI would not feel the emotion humans describe as 'hate' is, unless we specifically program that in.
Which, by the way, isn't just stupid, but far more difficult than it seems - emotions are very, very
complex mechanisms, and while it's possible to program it, it just isn't worth the effort.
Finally, the last thing is that AI will be better than us. Not just faster, but able to think in more efficient
ways, and use that power in ways that we probably wouldn't be able to think of, limited by very
specifically defined biological pathways that we are, and able to self-modify as they are. AI is not a
case of 'book-smarts', it's not the things you usually think of when you hear the word 'intelligent'(like
chess or math), but one that's better than us at everything - social skills, military strategy, even musical
skill.
## Emergence and Strange Loops
You have heard about neurons, right? The mysterious system in our brain is complex things but has a
very simple function. A single neuron does a few addition and subtraction in our brains and sends the
necessary signals for our body parts to function. It sounds simple but is complex as it controls the
entire functioning of a human body.
There is a concept called emergence, which is when small parts with simple functions combine to form
larger parts with more complex functions (like those neurons combining to form you), those large parts
would interact with each other in ways the smaller parts do not - like you reading this, but a single
neuron can’t read this. Intelligence is not just this, a random combination of neurons would not create
intelligence, and the process of coordinating them in highly specific ways is the more that is given to
the parts to form intelligence.
The AI algorithm running in a computer does the same procedure. At the core, a computer is a
machine with some on and off switches. Sequences of these form logic gates and some basic
functions. Sequences of these functions form operators which are coding languages like Python and
the math we implement using them. At no point in this process is there anything more than switches
going on and off, but the way they interact allows for increasingly complex things to happen.
Likewise, a very small group of neurons arranged in a specific way can handle more complexity than a
single neuron, as they now have several internal neurons that can process the input signals between
them before giving more than a single output signal.

## AI Alignment
Ever thought of killing an AI? Well, it proves to be not that easy. [Here is an introductory video about the
problems associated with trying to control or destroy an unfriendly AI.](https://www.youtube.com/watch?v=4nN8wUVZ9u0)
So controlling a machine is not as easy as creating one (well creating one is difficult though, but still..).
Controlling a machine that thinks in an entirely different way to us and maybe far more intelligent than
we are is challenging. Even if we have a perfect solution to the control problem we are left with a
second problem, what should we ask the AI to do, think, and value? The answer to this is AI Alignment.
Remember when we discussed a little bit about it in the first material, if not, just go through it once
again.
What we want are a set of rules or principles that an AI can refer to, to make a decision itself, knowing
that by following those rules its action will be aligned with what humans would want.
## Self-modifying AI
There's a saying among futurists that a human-equivalent artificial intelligence will be our last invention.
After that, AIs will be capable of designing virtually anything on their own-including themselves. That
sort of intelligence that can modify itself, is called a seed AI.

![alt text](https://github.com/allenabraham999/SiG/blob/main/images-2/2Img1.jpg)

When systems start to modify themselves, we have to be able to trust that all their modifications are
safe. This means that we need to know something about all possible modifications. But how can we
ensure that a modification is safe if no one can predict ahead of time what the modification will be?
There are two obvious solutions to this problem. The first option is to restrict a system’s ability to
produce other AI agents. However, we do not want to solve the safe self-improvement problem by
forbidding self-improvement! The second option, then, is to permit only limited forms of
self-improvement that have been deemed sufficiently safe, such as software updates or processor and
memory upgrades. Yet, vetting these forms of self-improvement as safe and unsafe is still complicated.
Regardless of what the ultimate solution is, successfully overcoming the problem of self-improvement
depends on AI researchers working closely together.

## Value Systems

AI works alongside diverse, human interests. People make decisions based on several factors,
including their experiences, memories, upbringing, and cultural norms. These factors allow us to have
a fundamental understanding of “right and wrong” in a wide range of contexts, at home, in the office, or
elsewhere. This is second nature for humans, as we have a wealth of experiences to draw upon.
Today’s AI systems do not have these types of experiences to draw upon, so it is the job of designers
and developers to collaborate to ensure consideration of existing values. So, instead of trying to control
an unfriendly AI, we have to make a friendly AI. And the way to do this is what we call a value system.

![alt text](https://github.com/allenabraham999/SiG/blob/main/images-2/2Img2.jpg)

With the ideal value system, the AI would not only find the most efficient ways to get what we want
with a surface-level interpretation of the tasks we give it, and we can't count on programmers being
good enough to express their entire values every single time-but also not modify itself in a way that
prevents those values from being changed. So what would this value system look like? Well, at its
core, it would look like a number whose value (ideally) corresponds to how much we like an outcome,
and the AI's motive would be to maximize that number. I add the disclaimer 'ideally' because it's not
difficult to imagine something like an AI told to fill a bucket that keeps adding more water to it even as it
starts to flow out and the room starts to flood and you start to drown because that will always increase
the probability that the bucket is 'full'. The problem is how we decide what that number represents, or
more specifically, how we train the algorithm to recognize that the number increases when it does
something to fulfill our true values. Now that is a question without a sufficiently good answer (yet).
That's one of the best things about this field. It doesn't require intense mathematical understanding or
great programming skills, just new ideas on how to approach it using what you know about the
problem. There are many proposed solutions, such as training the AI to infer what the hidden motive is
in a task, or training it to model a 'perfect human' to gauge its values, but those come with their
problems, both theoretical and implementational.

## Want to know more about AI?

AI Safety is a relatively new field. It's been around for just between one and two decades. This means
that it currently lies in its innovation phase, where the entire field is limited only by the pure number of
people coming up with ideas (as opposed to something larger, like AI, where the limitation is more
technical and less related to capital). If you want to learn more about Friendly AI, I recommend
understanding more about intelligence itself (Godel Escher Bach), a little bit more in the theory of stuff
like neural networks, and as much of stuff like decision theory.
[Find More About Decision theory here](https://en.wikipedia.org/wiki/Decision_theory)
